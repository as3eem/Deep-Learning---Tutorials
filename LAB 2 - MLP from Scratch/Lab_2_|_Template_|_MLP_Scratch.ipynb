{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Lab 2 | Template | MLP Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pDVndtX3dFK"
      },
      "source": [
        "# LAB 2 - MLP\n",
        "\n",
        "In this assignment, you need to implement a MLP Classfier Class for training a multi-layer perceptron on the given subset of MNIST dataset.\n",
        "\n",
        "--\n",
        "1. Create the neural network using the descriptions given below. Implement forward propagation and backward propagation from scratch on the constructed neural network. Use the following conditions for this question.\n",
        "\n",
        "\n",
        "*   Network Structure: Input 784, output 10. You are free to use any number of hidden layers with any number of neurons.\n",
        "*   Optimizer: Gradient-descent optimizer (must make batch_size parameter = length of entire dataset in this case).\n",
        "*   Activation: Show results using these activation functions for hidden layers: Sigmoid, Tanh, ReLU. Output layer will always have Softmax activation.\n",
        "*   Initialization: Random\n",
        "\n",
        "-- \n",
        "**Structure of the file:**\n",
        "\n",
        "Class to implemented: MLPClassifier\n",
        "\n",
        "Class functions:\n",
        "1. __init__(layers, learning_rate, activation_function, optimizer,\n",
        "weight_init, regularization, batch_size, num_epochs, dropouts, **kwargs)\n",
        "- layers: A list of number of neurons in each layer, starting from input layer,\n",
        "intermediate hidden layers and output layer. For example, a list consisting of [784, 200,\n",
        "50, 10] means the number of input features to the NN is 784, and it consists of 2 hidden\n",
        "layers with 200, 50 neurons in hidden layers 1 and 2, and it has 10 neurons in the output\n",
        "layer. Since we are using MNIST dataset, the number of neurons in the input and output\n",
        "layer will remain constant i.e. 784 for input and 10 for output. No default value is there,\n",
        "this is a necessary argument.\n",
        "- learning_rate: Learning rate of the neural network. Default value = 1e-5.\n",
        "- activation_function: A string containing the name of the activation function to be\n",
        "used in the hidden layers. For the output layer use Softmax activation function. Default\n",
        "value = “relu”.\n",
        "- optimizer: A string containing the name of the optimizer to be used by the network.\n",
        "Default value = “gradient_descent”.\n",
        "- Weight_init: “random”, “he” or “xavier”: String defining type of weight initialization\n",
        "used by the network. Default value = “random”.\n",
        "- Regularization: A string containing the type of regularization. The accepted values\n",
        "can be “l1”, “l2”, “batch_norm”, and “layer_norm” . The default value is “l2”.\n",
        "- Batch_size: An integer specifying the mini batch size. By default the value is 64.\n",
        "- Num_epochs: An integer with a number of epochs the model should be trained for.\n",
        "- dropout: An integer between 0 to 1 describing the percentage of input neurons to be randomly masked.\n",
        "- **kwargs: A dictionary of additional parameters required for different optimizers. By default it is None, however, you must initialize different parameters of optimizers with some valid input value for convergence. (This parameter will not be used in the test file)\n",
        "Output: (void)\n",
        "\n",
        "\n",
        "2. fit(X, Y)\n",
        "- X: a numpy array of shape (num_examples, num_features).\n",
        "- Y: a numpy array of shape (num_examples): This array contains the classification labels of the task.\n",
        "Output: (void)\n",
        "\n",
        "Note: This function should log the loss after some minibatches (you can choose this arbitrarily) and after the complete epoch.\n",
        "\n",
        "3. predict(X):\n",
        "- X: a numpy array of shape (num_examples, num_features)\n",
        "Output: numpy array of shape (num_examples) with classification labels of each class.\n",
        "\n",
        "4. predict_proba(X):\n",
        "- X: a numpy array of shape (num_examples, num_features)\n",
        "Output: numpy array of shape (num_examples, num_classes): This 2d matrix contains the probabilities of each class for all the examples.\n",
        "\n",
        "5. get_params():\n",
        "Output: An array of 2d numpy arrays. This array contains the weights of the model.\n",
        "\n",
        "6. score(X, y):\n",
        "- X: a numpy array of shape (num_examples, num_features): This 2d matrix contains the complete dataset.\n",
        "- Y: a numpy array of shape (num_examples): This array contains the classification labels of the task.\n",
        "Output: (float) Classification accuracy given X and y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uz-VMTt-O3J"
      },
      "source": [
        "class MLPCLassifier():\n",
        "    def __init__(layers, learning_rate, activation_function, \n",
        "                 optimizer, weight_init, regularization, \n",
        "                 batch_size, num_epochs, dropouts, **kwargs):\n",
        "\n",
        "        # -----\n",
        "        # layers: A list of number of neurons in each layer: Example -> [784, 200, 50, 10]\n",
        "        # learning_rate: Default value = 1e-5.\n",
        "        # activation_function: A string containing the name of the activation function. For the output layer use Softmax activation function. Default value = “relu”.\n",
        "        # optimizer: A string containing the name of the optimizer. Default value = “gradient_descent”.\n",
        "        # Weight_init: “random”, “he” or “xavier”. Default value = “random”.\n",
        "        # Regularization: A string containing the type of regularization. The accepted values can be “l1”, “l2”, “batch_norm”, and “layer_norm” . The default value is “l2”.\n",
        "        # Batch_size: An integer specifying the mini batch size. default value is 64.\n",
        "        # Num_epochs: An integer with a number of epochs the model should be trained for.\n",
        "        # dropout: An integer between 0 to 1 describing the percentage of input neurons to be randomly masked.\n",
        "        # **kwargs: A dictionary of additional parameters. By default it is None.\n",
        "        # Output: (void)\n",
        "        # ------\n",
        "        return \n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        # ------\n",
        "        # X: a numpy array of shape (num_examples, num_features).\n",
        "        # Y: a numpy array of shape (num_examples): This array contains the classification labels of the task.\n",
        "        # Output: (void)\n",
        "        # ------\n",
        "        return\n",
        "\n",
        "    def predict(self, X):\n",
        "        # ------\n",
        "        # X: a numpy array of shape (num_examples, num_features)\n",
        "        # Output: numpy array of shape (num_examples) with classification labels of each class.\n",
        "        # ------\n",
        "        return\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # ------\n",
        "        # X: a numpy array of shape (num_examples, num_features)\n",
        "        # Output: numpy array of shape (num_examples, num_classes): This 2d matrix contains the probabilities of each class for all the examples.\n",
        "        # ------\n",
        "        return\n",
        "\n",
        "    def get_params(self):\n",
        "        # ------\n",
        "        # Output: An array of 2d numpy arrays. This array contains the weights of the model.\n",
        "        # ------\n",
        "        return\n",
        "\n",
        "    def score(self, X, y):\n",
        "        # ------\n",
        "        # X: a numpy array of shape (num_examples, num_features): This 2d matrix contains the complete dataset.\n",
        "        # Y: a numpy array of shape (num_examples): This array contains the classification labels of the task.\n",
        "        # Output: (float) Classification accuracy given X and y.\n",
        "        # ------\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    # - - - - - - - - --\n",
        "    # HELPER FUNCTIONS -\n",
        "    # - - - - - - - - --\n",
        "\n",
        "    # ------ [Recommended] Forward And Backprop Functions you \"can\" use while completing above code -------\n",
        "    def forward_propagate(self, X,y):\n",
        "        # write your code here\n",
        "        return \n",
        "    \n",
        "    def cross_entropy_loss(self,output,label):\n",
        "        return # put your formula here\n",
        "    \n",
        "    def backpropagate(self):\n",
        "        # write your code here\n",
        "        return    \n",
        "\n",
        "    # ------ Extra functions (\"recommeneded\") that you might need in above coding ------\n",
        "    def relu(self,x):\n",
        "        # write your formula here\n",
        "        return \n",
        "    \n",
        "    def tanh(self,x):\n",
        "        # write your formula here\n",
        "        return \n",
        "\n",
        "    def softmax(self,x):\n",
        "        # write your formula here\n",
        "        return \n",
        "    \n",
        "    def derivative_tanh(self,x):\n",
        "        # write your formula here\n",
        "        return \n",
        "\n",
        "    def derivative_relu(self,x):\n",
        "        # write your formula here\n",
        "        return \n",
        "            \n",
        "    def derivative_sigmoid(self,x):\n",
        "        # write your formula here\n",
        "        return \n",
        "    \n",
        "    def activation(self,x,activation_function):\n",
        "        if activation_function == 'relu':\n",
        "            return self.relu(x)\n",
        "        elif activation_function=='sigmoid':\n",
        "            return self.sigmoid(x)\n",
        "        elif activation_function=='tanh':\n",
        "            return self.tanh(x)\n",
        "        elif activation_function==\"softmax\":\n",
        "            return self.softmax(x)\n",
        "        else:\n",
        "            return self.softmax(x)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKjjA_UDgkWP",
        "outputId": "4074a115-2ee8-4217-fa61-4abe0b4d0cff"
      },
      "source": [
        "# if you are using drive access then uncomment this\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuzVYigZEnPb"
      },
      "source": [
        "def load_train_test_data():\n",
        "    # change data path as per your avaiability\n",
        "    with open(\"./Data/train_set.pkl\",\"rb\") as train:\n",
        "        train_data = pickle.load(train)\n",
        "        train.close()\n",
        "    with open(\"./Data/val_set.pkl\",\"rb\") as val:\n",
        "        val_data = pickle.load(val)\n",
        "        val.close()\n",
        "    return train_data,val_data\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "train_data,val_data = load_train_test_data()\n",
        "for index,row in train_data.iterrows():\n",
        "    train_data.at[index,'Image'] = np.array(row['Image'])\n",
        "for index,row in val_data.iterrows():\n",
        "    val_data.at[index,'Image'] = np.array(row['Image'])\n",
        "\n",
        "\n",
        "train_X = np.stack(train_data['Image'].values)\n",
        "train_Y = train_data['Labels'].values\n",
        "val_X = np.stack(val_data['Image'].values)\n",
        "val_y = val_data['Labels'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnMNikz30sSG"
      },
      "source": [
        "Done?\n",
        "\n",
        "# How will you do this?\n",
        "\n",
        "\n",
        "Implement the following gradient descent optimizers from scratch. Use mini-batch size = 64. The following\n",
        "gradient descent optimizers are to be implemented:\n",
        "*  Gradient Descent with Momentum\n",
        "*  Nestrov’s Accelerated Gradient\n",
        "*  AdaGrad\n",
        "*  RMSProp\n",
        "*  Adam\n",
        "\n",
        "\n",
        "Implement the following weight initialization techniques from scratch and do a thorough analysis on the output of each one of them. \n",
        "* He\n",
        "* Xavier\n",
        "\n",
        "Implement the following regularizations from scratch \n",
        "\n",
        "* L1 Regularization\n",
        "* L2 Regularization\n",
        "* Dropouts\n"
      ]
    }
  ]
}