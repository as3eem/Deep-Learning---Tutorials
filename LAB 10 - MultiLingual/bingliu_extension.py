# -*- coding: utf-8 -*-
"""bingliu_extension.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nJJRhkTiNOJiQ99pyUeeqC57IqAlORbp
"""

# installations


from gensim.models import Word2Vec, KeyedVectors
import pandas as pd
import itertools

"""## EXTEND BINGLIU """

filename = 'data/english-hindi-dictionary.txt'

data_df = pd.read_table(filename, names=('en','sep','hi'), delim_whitespace=True)
data_df = data_df.drop(columns=['sep'])

data_dict = data_df.set_index('en').T.to_dict('list')


eng_hind_dict_set = set()
for key in data_dict:
  pair = (key, data_dict[key][0])
  eng_hind_dict_set.add(pair)

bingliu = pd.read_csv('data/BingLiu.csv', sep='\t', header=None)
bingliu.head()

bingliu=bingliu.rename(columns={0: "eng", 1: "lexicon"})
bingliu.head()

def add_hindi_word(row, data_dict=data_dict):
  eng = row['eng']
  if eng in data_dict: 
      hi = data_dict[eng][0]
  else: 
      hi = None  
  return hi

bingliu['hindi'] = bingliu.apply(add_hindi_word, axis=1)

bingliu.head()

bingliu.info()

bingliu = bingliu.dropna()  #drop rows with no hindi translation

bingliu = bingliu[bingliu['eng'] != bingliu['hindi']]  #keep rows where en != hi

bingliu.head()

#bingliu.to_csv('data/L1.csv', header=True)

"""DONE!

## WORD2VEC MODEL
"""

eng_w2v_model=Word2Vec.load("trained-models/400_stopwords_word2vec_eng.model")
# eng_w2v_model=Word2Vec.load("trained-models/50_word2vec_eng.model")

# eng_w2v_model = KeyedVectors.load_word2vec_format("pre-trained-models/GoogleNews-vectors-negative300.bin.gz", binary = True, limit = 100000)

def english_word2vec(word):
  # returns 5 word vectors based on our corpus
  try:
    result = eng_w2v_model.most_similar(word, topn = 5)
  except:
    result = [[0,0]]
  result.append([word, 0])
  return [word[0] for word in result]

hindi_w2v_model=Word2Vec.load("trained-models/400_stopwords_word2vec_hindi.model")
# hindi_w2v_model=Word2Vec.load("trained-models/50_word2vec_hindi.model")

# hindi_w2v_model = Word2Vec.load("pre-trained-models/hi.bin")

def hindi_word2vec(word):
  # returns 5 word vectors based on our corpus
  try:
    result = hindi_w2v_model.most_similar(word, topn = 5)
  except:
    result = [[0,0]]
  result.append([word, 0])
  return [word[0] for word in result]

from gensim.models.keyedvectors import KeyedVectors

import numpy as np
from gensim.scripts.glove2word2vec import glove2word2vec
_ = glove2word2vec("vectors-english-300.txt", "trained-models/300_glove_gensim_model_eng.model")
eng_glove_model = KeyedVectors.load_word2vec_format("trained-models/300_glove_gensim_model_eng.model", binary = False)

_ = glove2word2vec(glove_input_file="vectors-hindi-300.txt", word2vec_output_file="trained-models/300_glove_gensim_model_hindi.model")
hindi_glove_model = KeyedVectors.load_word2vec_format("trained-models/300_glove_gensim_model_hindi.model", binary=False)

def english_glove(word):
  try:
    result = eng_glove_model.most_similar(word, topn = 5)
  except:
    result = [[0,0]]
  result.append([word, 0])
  return [word[0] for word in result]

def hindi_glove(word):
  try:
    result = hindi_glove_model.most_similar(word, topn = 5)
  except:
    result = [[0,0]]
  result.append([word, 0])
  return [word[0] for word in result]

bingliu_extended_dict = dict()
for ind in bingliu.index:
  bingliu_extended_dict[(bingliu["eng"][ind], bingliu["hindi"][ind])] = bingliu["lexicon"][ind]

def extend_embeddings():
  extensions_added = True
  number_of_additions = 0
  bingliu_extensions = dict()
  while extensions_added:
    extensions_added = False
    for ind in bingliu.index: 
     eng, hindi = bingliu['eng'][ind], bingliu['hindi'][ind]
     # word2vec
     eng_neighbours = english_word2vec(eng)
     hindi_neighbours = hindi_word2vec(hindi)
     if eng_neighbours == 0 or hindi_neighbours == 0:
       continue
     pairs = itertools.product(eng_neighbours,hindi_neighbours)
     for pair in pairs:
       eng_word, hindi_word = pair[0], pair[1]
       if (eng_word, hindi_word) in eng_hind_dict_set and (eng_word, hindi_word) not in bingliu_extended_dict:
         print("adding", (eng_word, hindi_word), bingliu["lexicon"][ind])
         extensions_added = True
         bingliu_extended_dict[(eng_word, hindi_word)] = bingliu["lexicon"][ind]
         bingliu_extensions[(eng_word, hindi_word)] = bingliu["lexicon"][ind]
         number_of_additions += 1
  return number_of_additions, bingliu_extensions



addition_number, addition_entries = extend_embeddings()
print(addition_number)
addition_entries

def extend_embeddings_glove():
  extensions_added = True
  number_of_additions = 0
  bingliu_extensions = dict()
  while extensions_added:
    extensions_added = False
    for ind in bingliu.index: 
     eng, hindi = bingliu['eng'][ind], bingliu['hindi'][ind]
     # glove
     eng_neighbours = english_glove(eng)
     hindi_neighbours = hindi_glove(hindi)
     glove_pairs = []
     if eng_neighbours != 0 and hindi_neighbours != 0:
       glove_pairs = itertools.product(eng_neighbours,hindi_neighbours)
     for pair in glove_pairs:
       eng_word, hindi_word = pair[0], pair[1]
       if (eng_word, hindi_word) in eng_hind_dict_set and (eng_word, hindi_word) not in bingliu_extended_dict:
         print("adding", (eng_word, hindi_word), bingliu["lexicon"][ind])
         extensions_added = True
         bingliu_extended_dict[(eng_word, hindi_word)] = bingliu["lexicon"][ind]
         bingliu_extensions[(eng_word, hindi_word)] = bingliu["lexicon"][ind]
         number_of_additions += 1
  return number_of_additions, bingliu_extensions

addition_number, addition_entries = extend_embeddings_glove()
print(addition_number)
addition_entries

final_dict = {
    "eng" : [],
    "hindi" : [],
    "lexicon" : []
}
for key in bingliu_extended_dict:
  final_dict["eng"].append(key[0])
  final_dict["hindi"].append(key[1])
  final_dict["lexicon"].append(bingliu_extended_dict[key])

final_df = pd.DataFrame(final_dict)
final_df.to_csv("extended-lexicon.csv", index = True, header = True)